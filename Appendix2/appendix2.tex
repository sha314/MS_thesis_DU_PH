%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix B ********************************

\input{style.tex}


\chapter{Convolution}
\label{appendix.convolution}
Say we want to measure an observable $Q$, for example, for a specific occupied number of sites(bonds) in site(bond) percolation. But doing this we ignore the fact that we would have chose to fix a probability $p$ first and starting occupying the lattice sequentially then after we have had visited all the sites we would not have $p=n/(L^2)$ for site or $p=n/(2 L^2)$ for bond percolation each time. Therefore it would lead to some error. We can solve this dilemma by the following process. 
The trick \cite{Hu1992, Gould2006} is to measure $Q$ for fixed numbers of occupied sites (or bonds) $n$ in the range of interest. Let us refer to the ensemble of states of a percolation system with exactly $n$ occupied sites or bonds as a "microcanonical percolation ensemble," the number $n$ playing the role of the energy in thermal statistical mechanics. The more normal case in which only the occupation probability $p$ is fixed is then the "canonical ensemble" for the problem. (If one imagines the occupied sites or bonds as representing particles instead, then the two ensembles would be "canonical" and "grand canonical," respectively. Taking site
percolation as an example, the probability of there being
exactly $n$ occupied sites on the lattice for a canonical percolation ensemble is given by the binomial distribution
	\begin{equation}
		B(N,n,p) = {N\choose n} p^n (1-p)^{N-n}
	\end{equation}
	The same expression applies for bond percolation, but
	with $N$ replaced by $M$ , the total number of bonds.	Thus, if we can measure our observable within the microcanonical ensemble for all values of $n$, giving a set
	of measurements ${Q_n}$, then the value in the canonical
	ensemble will be given by
	\begin{equation}
		Q(p) 
		= \sum_{n=0}^{N} B(N,n,p) Q_n 
		= \sum_{n=0}^{N} {N\choose n} p^n(1-p)^{N-n} Q_n 	
	\end{equation}
	Thus we need only measure $Q_n$ for all values of $n$.
	\cite{Newman2001}.
	
	

	
\section{Algorithm}
	But using this formula to calculate $Q(p)$ is quite expensive since the number of sites(bonds) can be quite large  direct evaluation of the binomial coefficients using factorials is not possible. We use alternative way to measure $Q(p)$, which is basically does the same thing but in an efficient way.
 
	Instead, therefore, we recommend the following method of evaluation. The binomial distribution, Eq. (1), has its largest value for given $N$ and $p$ when $n = n_{max} = p N$ . We arbitrarily set this value to $1$. Now we calculate $B(N, n, p)$ iteratively for all other $n$ from
	
	\begin{equation*}
	B(N,n,p) = 
	\begin{cases}
		B(N, n-1, p) \frac{N-n+1}{n} \frac{p}{1-p}  \text{ if } n > n_{max}\\
		B(N, n+1, p) \frac{n+1}{N-n} \frac{1-p}{p}  \text{ if } n < n_{max}
	\end{cases}   
	\end{equation*}
	Then we calculate the normalization coefficient $C = \sum_{n} B(N, n, p)$ and divide all the $B(N, n, p)$ by it, to correctly normalize the distribution 	\cite{Newman2000, Newman2001}.
	
\section{Code}
	In order to use convolution the following function can be used. It is very efficient and uses OpenMP for parallel run, meaning, all the cores of the processors performs the same task in a divide-and-conquer manner. This function takes an array as input and returns the convoluted version as output.
	\begin{lstlisting}[style=CStyle]
	vector<double> convolution(vector<double>& data_in) {
	size_t N = data_in.size();
	std::vector<double> _forward_factor(N);
	std::vector<double> _backward_factor(N);
	for (size_t i=0; i < N; ++i)
	{
	_forward_factor[i]  = (double) (N - i + 1) / i;
	_backward_factor[i] = (double) (i + 1) / (N - i);
	}
	vector<double> data_out(N);
	long step = N / 1000;
	#pragma omp parallel for schedule(dynamic)
	for (long j=0; j <N; ++j) // start from j=1
	{
	double prob     = (double) j / N;
	double factor   = 0;
	double binom    = 0;
	double prev     = 0;
	double bn_tot   = 1; // normalization factor
	double sum      = data_in[j];
	// forward iteraion part
	factor = prob / (1-prob);
	prev   = 1;
	for (long i=j+1; i<N; ++i)
	{
	binom     = prev * _forward_factor[i] * factor;
	bn_tot += binom;
	sum      += data_in[i] * binom;
	prev      = binom;
	}
	// backward iteration part
	factor = (1-prob)/prob;
	prev   = 1;
	for (long i=j-1; i>=0; --i)
	{
	binom     = prev * _backward_factor[i] * factor;
	bn_tot += binom;
	sum      += data_in[i] * binom;
	prev      = binom;
	}
	// normalizing data
	data_out[j] = sum / bn_tot;
	cout << bn_tot << endl;
	}
	return data_out;
	}
	\end{lstlisting}
Complete code for convolution is available at 
\url{https://github.com/sha314/Convolution}.\\

The program linked above works using command line arguments. For example to perform convolution on a file "data.txt" which contains two columns out of which only second column needed to be convoluted use the following command.


\begin{lstlisting}[language=bash]
./convolute -f data.txt -a 0 -b 1
\end{lstlisting}

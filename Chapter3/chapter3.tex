%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Phase Transition}
\label{chapter3}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter3/Figs/thermodynamics/}{Chapter3/Figs/}{Chapter3/Figs/ising-model/}}
\else
    \graphicspath{{Chapter3/Figs/thermodynamics/}{Chapter3/Figs/}{Chapter3/Figs/ising-model/}}
\fi

The term phase transition is generally use to describe the transition between solid, liquid or gaseous states and in some cases the plasma state. It is one of the most studied problem in physics. Phase transition is a process where below a critical point the system behaves in one way whereas above that point the system behaves in a completely different way. There is a control parameter in phase transition. It can be temperature $T$ or magnetic field $H$. For example in ferromagnet to paramagnet transition temperature is the control parameter and for normal to superconductor transition both temperature and magnetic field are the control parameter.


\section{Classification}
	\subsection{Ehrenfest classification}
	Paul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables \cite{Jaeger1998}. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. 
	
	First-order phase transitions exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable \cite{Blundell2006}. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. 
	
	Second-order phase transitions are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy \cite{Blundell2006}. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions.
	
	Though useful, Ehrenfest's classification has been found to be an incomplete method of classifying phase transitions, for it does not take into account the case where a derivative of free energy diverges (which is only possible in the thermodynamic limit). For instance, in the ferromagnetic transition, the heat capacity diverges to infinity. The same phenomenon is also seen in superconducting phase transition.
	
	\subsection{Modern classifications}
	In the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\cite{Jaeger1998}
	
	First-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not. Familiar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). 
	
	Second-order phase transitions are also called continuous phase transitions. They are characterized by a divergent susceptibility, an infinite correlation length, and a power-law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state—mixed-state and mixed-state—superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature \cite{Ojovan2013} which enables accurate detection using differential scanning calorimetry measurements
	
	\subsection{Basic Properties of the classes}
	Some basic properties of the two classes of phase transition is listed below.
	\paragraph{First Order}
		\begin{enumerate}
			\item Latent heat of nucleation in growth
			\item Symmetry may or may not be broken
			\item Discontinuous change in entropy
		\end{enumerate}
	
	\paragraph{Second Order}
		\begin{enumerate}
			\item No Latent heat or meta-stable state
			\item Sysmmetry is always broken
			\item Continuous change in entropy
		\end{enumerate}
	At the event of critical point of phase transition there might exists a meta-stable state, where both phases exists simultaneously. At the meta-stable state the control parameter or temperature (in case of thermal phase transition, e.g., ice to water or water to vapor) does not change but there is a change (usually large) in heat which give rise to discontinuity of the first derivative of free energy, i.e., entropy \ref{subsect:entropy-thermodynamics} \ref{subsect:percolation-entropy}. And if the entropy is continuous then the latent heat is zero which is the signature of second order transition.

	
	
\section{Thermodynamic Quantities}
	The first explicit statement of the first law of thermodynamics, by \textit{Rudolf Clausius} in 1850, referred to cyclic thermodynamic processes.
	
	
	\textit{In all cases in which work is produced by the agency of heat, a quantity of heat is consumed which is proportional to the work done; and conversely, by the expenditure of an equal quantity of work an equal quantity of heat is produced.}
	
	\begin{align}
		\Delta E = Q + W
	\end{align}
	where, $Q$ is the net quantity of heat supplied to the system by its surroundings and $W$ is the net work done by the system.
	The IUPAC convention for the sign is as follows: All net energy transferred to the system is positive and net energy transferred from the system is negative.
	Clausius also stated the law in another form, referring to the existence of a function of state of the system, the internal energy, and expressed it in terms of a differential equation for the increments of a thermodynamic process.
	
	\textit{In a thermodynamic process involving a closed system, the increment in the internal energy is equal to the difference between the heat accumulated by the system and the work done by it.}
	
	For quasi-static process
	\begin{align}
		dU &= \delta Q - \delta W \label{def:enternal-energy}\\
		  &= dQ - P dV
	\end{align}
	$U$ is the internal energy.
	here $W = -P dV$
	since work done by the system on the environment if the product $P dV$ whereas the work done on the system is $-P dV$ for pressure $P$ and volume change $dV$.\\
	The term heat for $Q$ means "that  amount of energy added or removed by conduction of heat or by thermal radiation", rather than referring to a form of energy within the system. 
	The internal energy is a mathematical abstraction that keeps account of the exchanges of energy that befall the system.
	
	For quasi-static state we can write
	\begin{equation}
	dQ = T dS
	\label{eqn:def_enthalpy}
	\end{equation}
	where $S$ is the entropy of the system and $T$ is the temperature.
	Thus we can write for canonical ensemble
	\begin{equation}
	dU = TdS - PdV
	\label{def:internal-energy--entropy-volumn}
	\end{equation}
	such that $E=E(S,V)$  and 
	for grand canonical ensemble 
	\begin{equation}
	dU = TdS - pdV + \mu dN
	\end{equation}
	where $U=U(S,V,N)$.
	But a problem arises, since there is no device we currently posses that can measure entropy. So we use Legendre transformation to change variable dependency
	\begin{align}
		dU  	&= TdS - pdV  \nonumber \\ 
		&= TdS + SdT - SdT - PdV  \nonumber \\ 
		d(U-TS) &= -SdT - PdV  \nonumber \\
		dA 		&= -SdT - PdV \label{eqn:helmholtz_def}
	\end{align}
	where $A=A(T,V)$  is the Helmholtz free energy. We can perform another Legendre transformation in \ref{eqn:helmholtz_def} as follows
	\begin{align}
		dA 		&= -SdT - PdV -VdP + VdP \nonumber \\
		d(A+PV) &= -SdT + VdP \nonumber \\
		dG      &= -SdT + VdP \label{eqn:gibbs_def}
	\end{align}
	where $G=G(T,P)$ is the Gibbs free energy.
	
	
	\subsection{Entropy}
	\label{subsect:entropy-thermodynamics}
	One of the most important concept in Statistical Mechanics is entropy. The notion of entropy was first introduced in physics by a German scientist Rudolf Clausius who laid the foundation for the second law of thermodynamics in 1850 by examining the relation between heat transfer and work.
	The second law of thermodynamics states that the total entropy of an isolated system can never decrease over time. Therefore the direction toward which entropy increases monotonically is the direction of time. Monotonically means that it can increase or keep constant but never decrease.	The term entropy	is used in many other branches of science, sometimes distant from physics or mathematics (such	as sociology), where it no longer maintains its rigorous quantitative character. Usually, it roughly	means disorder, chaos, decay of diversity or tendency toward uniform distribution of kinds \cite{Downarowicz2009}.
	
	
	
	Entropy is considered as a quantity about the disorderness of a system. This kind of disorder is the number of states a system can take on. So what are the states of a system? Imagine a cube of volume $1cm^3$, filled with one particular gas. At a particular time if we can label all the molecules of the gas uniquely then at next moment most of the molecule will change their positions due to their random motion. Then we will not be able to identify each molecule with their previous label. This process of identifying the labels of the molecules are easier if it's a liquid and even more easier in it's solid form. Since temperature increases the random motion of the molecules, as the temprature rises it is more difficult to identify those molecules. Thus at high temperature a system has higher entropy. Another thing to mention about it's volume. If a larger volume is selected then obviously the number of possible states will increase therefore entropy will increase.
	
	
	\paragraph{Example}
	If we were ot compare the entropy of the moon and the sun, the above discussion tells us that the sun has higher entropy than the moon. The reason is that the sun is much much larger than the moon and has much higher temperature than the moon. Therefor the number of possible states will be larger for the sun than the moon.
	
	
	In Classical Physics, entropy is seen as a magnitude which in every time is proportional to	the quantity of energy that at that time cannot be transformed into mechanical work. Using the	above interpretation, entropy plays a central role in the formulation of the second law of thermodynamics which states that in an isolated physical system, any transformation leads to an increase	of its entropy. 
	
	In Probability Theory, the entropy of a random variable measures the uncertainty over the	values which can be reached by the variable.
	
	In Information Theory, the entropy of the compression of a message (for example, of a file	from a computer), quantifies the content of the information of the message to have the minimum	lost of information in the compression process previous to its transmission.
	
	In Abstract Theory of Dynamical Systems, the entropy measures the exponential complexity	of the system or the average flow of information per unit of time \cite{Balibrea2016}.
	
	So it is already explicit that the concept of entropy is not easy to grasp and frequently entropy is seen as very mysterious quantity and that’s why it received a very large number of interpretations,
	explications, applications. In order to understand how the complex concept of entropy emerged,
	in this chapter we will give a brief history and will review the works of Clausius, Boltzmann,
	Shannon and R\'{e}nyi.

	\subsubsection{Clausius Entropy}	
	As mentioned Earlier in this section The concept and name of entropy originated in the early 1850s in the work of Rudolf Julius Emmanuel Clausius $(1822-1888)$ and that work was at first	primarily concerned with the question of which cycle is best suited for the conversion of heat into	work and which substance is best used for the conversion \cite{Clausius1867}.
		
	Clausius based his argument on the plausible axiom that heat cannot pass by itself from a cold to a hot body. In order to exploit that axiom Clausius considered two competing Carnot cycles (see figure	\ref{fig:carnot-engines}) working in the same temperature range, one as a heat engine and one as a refrigerator;	the refrigerator consumes the work the engine provides. By comparing the amounts of heat $Q$	passed from top to bottom and vice versa, he came to the conclusion that among all efficiencies	the efficiency of a Carnot cycle is maximum and universal.	'Maximum' means that no cycle in the same range of temperature has a bigger efficiency than a	Carnot cycle, and 'universal' means that all working substances provide the same efficiency in a	Carnot cycle \cite{Carnot1890}.	It is easy to calculate the efficiency of the Carnot engine of an ideal gas:
	\begin{figure}
		\centering
		\includegraphics[width=10cm]{{{carnot-cycle-heat-engine-refrigerator}}}
		\caption{Two competing Carnot engines, pressure (P) versus volume (V) - diagrams \cite{Greven2003}}
		\label{fig:carnot-engines}
	\end{figure}
	\begin{align}
		\eta &= \frac{W}{Q_{boiler}} \nonumber\\
 			 &= \frac{Q_{boiler} - Q_{cooler}}{Q_{boiler}} \nonumber\\
 			 &= 1 - \frac{Q_{cooler}}{Q_{boiler}} \\
 			 &= 1 - \frac{T_{cooler}}{T_{boiler}}
 	\label{eqn:efficiency-carnot}
	\end{align}
	where $T$ is the Kelvin temperature. And, since by Clausius’s result this efficiency is universal, it	holds not only for ideal gases but for all substances, be they water, mercury, sulphur, or steel.
	Nicolas L\'{e}onard Sadi Carnot anticipated Clausius by 30 years, but no one could understand	Carnot's reasoning. Carnot believed in the caloric theory of heat, by which the heat passing through	the cycle from boiler to cooler is unchanged in amount. This is quite wrong and it is a kind of miracle that Carnot, despite his erroneous concepts, obtained a correct result. Carnot's trouble was	that he did not know the balance of energy, or the first law of thermodynamics, which states that
	\begin{align}
		dU &= dQ - dW \\
  		   &= dQ - P dV
	\end{align}
	since
	\begin{equation}
		dW = P dV
	\end{equation}
	This equation holds, if the work is expended reversibly for the volume change. With that superior	knowledge Clausius showed that it is not the heat that passes through the cycle from boiler to	cooler unchanged in amount. He proved that from equation \ref{eqn:efficiency-carnot}
	\begin{equation}
		\frac{Q}{T}\at[\Big]{\text{boiler}} =
		\frac{Q}{T}\at[\Big]{\text{heater}}
	\end{equation}
	nd decided to define the entropy change to be the ratio of heat flow to temperature: \cite{Sethna2006}
	\begin{equation}
		\Delta S_{\text{thermo}} = \frac{Q}{T}
	\end{equation}
	He also showed that in an arbitrary process, the change of entropy, satisfies the inequality
	\begin{equation}
		dS \geq \frac{dQ}{T}
		\label{eqn:2nd-law-of-themodynamics}
	\end{equation}
	This important relation is known as the second law of thermodynamics. We will now discuss the significance of this law.
	First, we stick to reversible process, where the equality holds in equation \ref{eqn:2nd-law-of-themodynamics}. We may then eliminate $dQ$ between the first and second laws, we obtain the Gibbs equation
	\begin{equation}
		dS = \frac{1}{T} \left(dU + P dV\right)
	\end{equation}
	In an adiabatic irreversible process, $dQ = 0$ and equation \ref{eqn:2nd-law-of-themodynamics} one has the fundamental relation between entropy and irresponsibility. In words
	
	\textbf{In a closed system (adiabatic), the entropy cannot decrease, it remains constant or increase \cite{Benguigui2013}}
		
	But the irreversible increase of entropy is not a property of the microscopic laws of nature. Because the microscopic laws of nature are time-reversal invariant. For example the Maxwell's equations are time reversible or the laws governing the motion of electrons of atoms are also time reversible. Thus the direction of time cannot be determined from microscopic laws of nature. But since entropy always increase monotonically, the direction in which entropy increases (or remains constant) is the direction of time \cite{Sethna2006}.
	
	
	Also by differentiating equation \ref{eqn:gibbs_def} with respect to $T$ keeping $P$ constant we can get an expression for entropy in terms of the Gibbs free energy.
	\begin{equation}
		S = -\frac{dG}{dT}
	\end{equation}
	similar expression is obtained from \ref{eqn:helmholtz_def}. Thus entropy is indeed the first derivative of free energy with respect to the control parameter $T$.
	
	
	%----------------------------------------------
		\subsubsection{Boltzmann Entropy}
		Another intuitive interpretation of entropy is as a measure of the disorder in a system.  Liquids have higher entropy	than crystals intuitively because their atomic positions are less orderly \cite{Sethna2006}. Ludwig Boltzmann	interpreted the entropy function as statistical entropy using probability theory.	Around 1900 there was a fierce debate going on between scientists whether atoms really existed	or not. Boltzmann was convinced that they existed and realized that models that relied on atoms	and molecules and their energy distribution and their speed and momentum, could be of great	help to understand physical phenomena. Because atoms where supposed to be very small, even	in relatively small systems, one faces already a tremendous number of atoms. For example: one
		mole of water contains about $6.023 \times 10^{23}$ molecules! Clearly it is impossible to track the energy	and velocity of each individual atom. Therefore, Boltzmann introduced a mathematical treatment	using statistical mechanical methods to describe the properties of a given physical system (for example the relationship between temperature, pressure and volume of one liter of air). 
		
		Boltzmann's	idea behind statistical mechanics was to describe the properties of matter from the mechanical	properties of atoms or molecules. In doing so, he was finally able to derive the Second Law of	Thermodynamics around 1890 and showed the relationship between the atomic properties and the	value of the entropy for a given system. It was Max Planck who formulated the expression of entropy of the ideal gas system based on Boltzmann’s results \cite{Schmitz2007}. The Boltzmann entropy formula	or Boltzmann-Planck entropy formula is
		\begin{equation}
			S = k_B \log\Omega
			\label{eqn:boltzmann-entropy}
		\end{equation}
	
		$S$ is called the Planck entropy or Boltzmann entropy. Here $k_B$ is the Boltzmann constant
		(ideal gas constant $R$ divided by Avogadro's number $N$) which equals to $1.4 \times 10^{-23}$ $J/K$), and $\Omega$, comes from the German \textit{Wahrscheinlichkeit}, meaning probability, which is often referred to as	disorder. In another way we can define it as the number of microstates (often modeled as quantum	states) with the given macrostate. Macrostate is any particular arrangement of atoms where we	look only on average quantities. Any individual arrangement defining the properties (e.g. positions	and velocities) of all the atoms for a given macrostate is a microstate. For a microstate it matters	what individual particles do, for the macrostate it does not.
		
		
		The logarithm is used because it simplifies the computations and reproduced the property of additivity of entropy, in the sense that the entropy of two systems sums instead of multiplies. In	equation \ref{eqn:boltzmann-entropy}, the entropy $S$ increases when $\Omega$ increases. More microstates give raise to more disorder	hence higher entropy. Besides for only one possible microstate, entropy is zero. The notion of	disorder is an intuitive notion depending of the system to be considered. In the case of a gas, it	is considered it in a ordered state if its molecules have a distribution of energies or positions very	
		different of those random which means the Boltzmann distribution. The most disordered states are	the most probable and as consequence have the most entropy. Another consequence is that in the	universe the disorder tends to increase \cite{Balibrea2016}.
		
		
		From there, it is a short conceptual jump to the second Law (total entropy tends to increase, and	all spontaneous processes increase entropy). In summary, the thermodynamic definition of entropy provides the experimental definition of entropy, while the probabilistic definition of entropy	extends the concept, providing an explanation and a deeper understanding of its nature \cite{Alam2016}.
		
		These definitions of entropy is of no use in our model. However we can use the Boltzmann entropy with a few modification to get Shannon entropy \cite{Shannon1948}. This process of getting Shannon entropy from Boltzmann entropy and how does it help us is described in section \ref{subsect:percolation-entropy}.

	
	\subsection{Specific Heat}
	Heat capacity is a physical quantity which is the ratio of the heat added to an object to the resulting temperature change \cite{Walker2013}. Note that removing heat is just like adding negative heat. The unit of heat capacity if joule per kelvin $j/K$. And it's dimensional form is $L^2MT^{-2}\Theta^-1$.	The specific heat is the amount of heat per unit mass ($1kg$) required to raise the temperature by one kelvin($1K$). 
	
	
	Heat capacity is an \textit{extensive} property of matter (it is proportional to the size of the system). When expressing the same phenomenon as an \textit{intensive} property, the heat capacity is divided by the amount of substance, mass, or volume, thus the quantity is independent of the size or extent of the sample. The molar heat capacity is the heat capacity per unit amount (SI unit: mole) of a pure substance, and the specific heat capacity, often called simply specific heat, is the heat capacity per unit mass of a material. In some engineering contexts, the volumetric heat capacity is used.
	
	Temperature represents the average randomized kinetic energy of constituent particles of matter relative to the centre of mass of the system, while heat is the transfer of energy across a system boundary into the body other than by work or matter transfer. Translation, rotation, and vibration of atoms represent the degrees of freedom of motion which classically contribute to the heat capacity of gases, while only vibrations are needed to describe the heat capacities of most solids, as shown by the Dulong–Petit law \cite{Kittel2004}.
	
	
	The internal energy of a closed system changes by adding heat to the system or by the system performing work.
	\begin{equation}
		\Delta E_{system} = E_{in} - E_{out}
	\end{equation}
	which is same expression as \ref{def:enternal-energy}. If the heat is added at constant volume, then the second term of this relation vanishes to give
	\begin{equation}
		C_V = \left(\frac{\partial U}{\partial T}\right)_V = \left(\frac{\partial Q}{\partial T}\right)_V
	\end{equation}
	Another useful quantity is the heat capacity at constant pressure, $C_P$. This quantity refers to the change in the enthalpy of the system given by
	\begin{equation}
		H = U + PV
	\end{equation}
	Thus the change in enthalpy is 
	\begin{align}
		dH 
		&= dU + d(PV) \nonumber \\
		&= dQ + dW + PdV + VdP \nonumber \\
		&= dQ  + VdP
	\end{align}
	Therefore at constant pressure we have
	\begin{equation}
		C_P = \left(\frac{\partial H}{\partial T}\right)_P = \left(\frac{\partial Q}{\partial T}\right)_P
	\end{equation}
	In general we can write the relationship between heat and temperature change as
	\begin{equation}
		C = \frac{Q}{dT}
	\end{equation}
	 where $C$ is the specific heat.
	and we know
	\begin{equation}
		Q = T dS
	\end{equation}
	we immediately get
	\begin{equation}
		C = T \frac{dS}{dT}
		\label{def:specific-heat-thermodynamics}
	\end{equation}
	Specific heat is called the response function for a non-magnetic system.
		
		
	\subsection{Order Parameter}
	An order parameter is a measure of the degree of order across the boundaries in a phase transition system. It's numerical usually ranges between $[0,1]$. At the critical point, the order parameter susceptibility will usually diverge.
	
	An example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid-gas transitions, the order parameter is the difference of the densities.
	
	Order parameter does have a theoretical perspective. It arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. Order parameter can be a function of more than one variable. In other words it can have more than one degree of freedom. It can also be defined for non-symmetry-breaking transitions. 

	
	\subsection{Susceptibility}
	The first law of thermodynamics for a non magnetic system is obtainable from \ref{def:internal-energy--entropy-volumn} using a conversion.	For a magnetic system $P \rightarrow h$ and $V \rightarrow -m$, where $h$ is the magnetic field and $m$ is the magnetization. The negative sign for the magnetization is because of the fact that the magnetization increases with the increase of the magnetic field whereas the volume decreases as the pressure increases. Since we relate pressure to the magnetic field, to relate volume to magnetization we need to put a minus sign with it. Thus we get
	\begin{align}
		dU &= TdS + hdm \\
		   &= TdS + SdT - SdT + h dm \\
		d(U - TS)  &= -SdT + h dm \\
		dA &= -SdT + h dm \label{eqn:helmholtz_mag_system} \\
		dA &= -SdT + h dm + m dh -m dh \\
		d(A - mh) &= -SdT - m dh\\
		dG &= -SdT -m dh \label{eqn:gibbs_mag_system}
	\end{align} 
	where $A=A(T,m)$ is the  Helmholtz free energy and $G=G(T,h)$ is the Gibbs free energy for the magnetic system. From the definition of free energy for a magnetic system \ref{eqn:gibbs_mag_system} \ref{eqn:helmholtz_mag_system} we can find the expression for entropy and magnetization in terms of free energy.
	\begin{equation}
		S = -\left(\frac{\partial G}{\partial T}\right)_h = -\left(\frac{\partial A}{\partial T}\right)_m
	\end{equation}
	\begin{equation}
		m = -\left(\frac{\partial G}{\partial h}\right)_T
		\label{eqn:magnetization_def}
	\end{equation}
	\begin{equation}
		h = \left(\frac{\partial A}{\partial m}\right)_T
		\label{eqn:mag_field_def}
	\end{equation}	
	And since the susceptibility is defined as the derivative of the magnetization with respect to the magnetic field, we get
	\begin{equation}
		\chi = \left(\frac{\partial m}{\partial h}\right)_h = - \left(\frac{\partial^2G}{\partial h ^2}\right)_T
		\label{eqn:susceptibility_def}
	\end{equation}
	again we can write $\chi$ as
	\begin{align}
		\chi &= \frac{1}{\frac{\partial h}{\partial m}} \nonumber \\
			 &= \frac{1}{\frac{\partial^2 A}{\partial m ^2}} \label{eqn:susceptibility_def2}
	\end{align}
	Susceptibility is called the response function for a magnetic system.
	
	
	%-------------------------------------------------------------------------------------------
\section{Shapes of the Thermodynamic Quantities}
	\subsection{Calculus to determine the shapes}
	\label{sec:determine_shape_using_calculus}
	From the law of Calculus we can estimate the approximate shape of a function by it's first and second derivative.
	Say we have a function $f(x)$ and we want to estimate it's shape.
	If the first derivative of this function is negative (positive) the function is said to have decreasing (increasing) slope \ref{fig:diff1_of_f}.
	\begin{figure}
		\centering
		\begin{subfigure}{0.329\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{thermodynamics/decreasing}}}
			\caption{negative slope}
		\end{subfigure}
		\begin{subfigure}{0.329\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{thermodynamics/increasing}}}
			\caption{positive slope}
		\end{subfigure}
		\caption{shape of $f(x)$ from $f^{\prime}(x)$}
		\label{fig:diff1_of_f}
	\end{figure}
	
	If the second derivative of this function is negative (positive) the function is said to have concave (convex) shape \ref{fig:diff2_of_f}.
	\begin{figure}
		\centering
		\begin{subfigure}{0.329\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{thermodynamics/concave}}}
			\caption{concave}
		\end{subfigure}
		\begin{subfigure}{0.329\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{thermodynamics/convex}}}
			\caption{convex}
		\end{subfigure}
		\caption{shape of $f(x)$ from $f^{\prime\prime}(x)$}
		\label{fig:diff2_of_f}
	\end{figure}
	Thus if we know the sign of the first and second derivative we can approximate a shape of the function.
	
	
	\subsection{Free Energy}
	Now since we obtain from \ref{eqn:gibbs_def} that the entropy is the first derivative of the free energy \ref{eqn:entropy_by_gibbs} and from \ref{eqn:def2_specific_heat} specific heat is the second derivative of the free energy \ref{eqn:specific_heat_by_gibbs}. Since the entropy is a positive quantity and specific heat is also a positive quantity we get that the first and second derivative of the free energy is negative which implies from the laws of calculus that the shape of the free energy should be a decreasing concave curve \ref{fig:free_energy} in other words concave shaped with negative slope.
	\begin{equation}
	S = - \frac{\partial G}{\partial T}
	\label{eqn:entropy_by_gibbs}
	\end{equation}
	\begin{equation}
	C = - \frac{\partial^2 G}{\partial T ^2}
	\label{eqn:specific_heat_by_gibbs}
	\end{equation}
	\begin{figure}
		\centering
		\includegraphics[width=0.3\columnwidth]{{{thermodynamics/free_energy}}}
		\caption{Shape of the free energy}
		\label{fig:free_energy}
	\end{figure}

	\subsection{Entropy and Specific Heat}	
	Now that we know the shapes of the free energy, the very next thing to do is to find the shape of the entropy. Since we can get entropy from differentiating the free energy with respect to temperature $T$ we get the following shape \ref{fig:entropy_1_2}. From the definition of the transition order we know that the first order transition is discontinuous and the second order transition continuous. Since the first order transition requires latent heat at the critical point the discontinuity is inevitable. Note that, since entropy measures disorderedness of a system it should increase with increasing temperature. Because the temperature is nothing but the average kinetic energy of the particles in the system. As the kinetic energy of the system increases with temperature, the particles tend to vibrate more and get higher average velocity. Thus making the system disordered. Therefore in a physical system entropy always increases with the increasing of temperature. If we find something different, as if, the entropy is decreasing with the increasing of temperature, there must be a problem somewhere.
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.329\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{thermodynamics/entropy_1st}}}
			\caption{Entropy in first order transition}
		\end{subfigure}
		\begin{subfigure}[t]{0.329\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{thermodynamics/entropy_2nd}}}
			\caption{Entropy in 2nd order transition}
		\end{subfigure}
		\caption{Shape of entropy}
		\label{fig:entropy_1_2}
	\end{figure}
	Just after knowing the shape of entropy one can anticipate the shape of specific heat, which is the derivative of the entropy with respect to temperature and is defined in \ref{eqn:def2_specific_heat_by_entropy}. Using this and the shape of entropy we can get the shape of the specific heat immediately as follows
	\begin{figure}
		\centering
		\includegraphics[width=8cm, height=8cm]{{{thermodynamics/specific_heat}}}
		\caption{Shape of Specific Heat and Susceptibility}
		\label{fig:specific_heat-susceptibility}
	\end{figure}
	\subsection{Order Parameter and Susceptibility}
	Since diamagnetic substances have negative susceptibilities $(\chi < 0)$; paramagnetic, and ferromagnetic substances have positive susceptibilities $(\chi > 0)$, and we are working for a phase transition model similar to paramagnet and ferromagnet transition, taking susceptibility to be positive is appropriate for our model. Now if susceptibility is positive then from \ref{eqn:susceptibility_def} we get $\left(\frac{\partial^2G}{\partial h ^2}\right) < 0$ which means that the shape of the free energy for this case is concave from the knowledge of \ref{sec:determine_shape_using_calculus}. And from \ref{eqn:magnetization_def} and the fact that the magnetic field $h$ can be positive or negative, as it can change direction, the Gibbs free energy is negative, since it is the lowest binding energy. \\
	Also from \ref{eqn:susceptibility_def2} we can say $\frac{\partial^2 A}{\partial m ^2}$ is positive giving convex shape of the Helmholtz free energy. And \ref{eqn:mag_field_def} tells us that since $h$ can ve positive or negative, the Helmholtz free energy can have increasing or decreasing slope respectively.
	


%------------------------------------------------------------------------------------
\section{Critical Exponents}
	Near the critical point there is, in general, a function that describes the behavior of the system that is mostly interesting. For thermodynamical system the temperature is the control parameter , thus that function depends on the temperature. But since we want the information at the critical point, $(T-T_c)/T_c$, instead of $T$ is a better parameter to address. In equation
	\begin{equation}
		\epsilon = \frac{T-T_c}{T_c} = \frac{T}{T_c} - 1
	\end{equation}
	$\epsilon$ is considered a better parameter. Thus a function $f(\epsilon)$ is used instead of $f(T)$. Using the fact that in the vicinity of $T_c$ the function $f(\epsilon)$ exhibits power law
	\begin{equation}
		f(\epsilon) \sim \epsilon^\lambda
	\end{equation}
	The following figures shows some function that exhibit power law
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{power_law_functions/fig1}}}
			\caption{$f(\epsilon) \sim \epsilon^\lambda$ where $\lambda=-1,-3/2$}
		\end{subfigure}
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth]{{{power_law_functions/fig2}}}
			\caption{$f(\epsilon) \sim \epsilon^\lambda$ where $\lambda=1/2,1/3$}
		\end{subfigure}
		\caption{Function showing power law}
	\end{figure}
	To see this more closely, we can expand any function $f(\epsilon)$ as a power series of $\epsilon$
	\begin{equation}
		f(\epsilon) = A \epsilon^\lambda ( 1 + B \epsilon^a + C \epsilon^b + ...)
	\end{equation}
	Near $T_c$ only the first term is dominating.
	We had the Gibbs free energy (for a magnetic system)
	\begin{equation}
		G = G(T,h) \equiv G(\epsilon, h)
	\end{equation}
	since $\epsilon$ is a better parameter. Assuming that $G$ is a generalized Homogeneous function. Therefore from the realization of section \ref{sect:homogenious-function} we can write
	\begin{equation}
		G(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h) = \lambda G(\epsilon, h)
		\label{eqn:gibbs_homogenious}
	\end{equation}
	differentiating equation \ref{eqn:gibbs_homogenious} with respect to $h$
	\begin{align}
		\frac{\partial G(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h)}{\partial h} &= \lambda \frac{\partial G(\epsilon, h)}{\partial h} \nonumber \\
		\frac{\partial G(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h)}{\partial \lambda^{a_h} h} \lambda^{a_h} &= \lambda \frac{\partial G(\epsilon, h)}{\partial h} \nonumber \\
		\lambda^{a_h} G^\prime(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h) &= \lambda G^\prime (\epsilon, h) \nonumber \\
		-\lambda^{a_h} m(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h) &= -\lambda m(\epsilon, h) \nonumber \\
		m(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h) &= \lambda^{1-a_h} m(\epsilon, h) \\
		\label{eqn:order_parameter_homogeneous}
	\end{align}
	Here $m$ is the magnetization or order parameter. The figure \ref{fig:shape-order-parameter} shows the general shape of the order parameter.
	\begin{figure}
		\centering
		\includegraphics[width=10cm, height=10cm]{{{order-parameter}}}
		\caption{General shape of Order Parameter}
		\label{fig:shape-order-parameter}
	\end{figure}

	Setting $h=0$ in equation \ref{eqn:order_parameter_homogeneous}
	\begin{align}
		m(\lambda^{a_\epsilon} \epsilon)	 &= \lambda^{1-a_h} m(\epsilon) \nonumber \\
		m(1)	 &= \lambda^{1-a_h} m(\epsilon) \nonumber \\
		m(1)	 &= \epsilon^{-\frac{1-a_h}{a_\epsilon}} m(\epsilon) \nonumber \\
		m(\epsilon) &= \epsilon^\beta m(1) \label{eqn:order_parameter_and_beta}
	\end{align}
	where
	\begin{equation}
		\beta = \frac{1-a_h}{a_\epsilon}
		\label{eqn:beta}
	\end{equation}
	Note that, setting $\lambda^{a_\epsilon} \epsilon = 1$ gives $\lambda = \epsilon^{-1/a_\epsilon}$.\\
	Setting $\epsilon=0$ in equation \ref{eqn:order_parameter_homogeneous}
	\begin{align}
		m(\lambda^{a_h} h) &= \lambda^{1-a_h} m(h) \nonumber \\
		m(1) &= h^{-\frac{1-a_h}{a_h}} m(h) \nonumber \\
		m(h) &= m(1) h^\delta \label{eqn:order_parameter_and_delta}
	\end{align}
	where
	\begin{equation}
		\delta = \frac{a_h}{1-a_h}
		\label{eqn:delta}
	\end{equation}
	Again, note that, setting $\lambda^{a_h} h = 1$ gives $\lambda = h^{-1/a_h}$\\
	Now, Since the response functions are the second derivative of the free energy \ref{sect:response-functions}, by differentiating \ref{eqn:gibbs_homogenious} twice with respect to $\epsilon$ we get the Specific Heat
	\begin{align}
		\lambda^{2 a_\epsilon} G^{\prime\prime}(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h) &= \lambda G^{\prime \prime}(\epsilon, h) \nonumber \\
		\lambda^{2 a_\epsilon} \frac{\partial G(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h)}{\left(\frac{1}{T_c}\right)^2 \partial T^2} &= \lambda G^{\prime \prime}(\epsilon, h) \nonumber \\
		\lambda^{2 a_\epsilon} C(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h) &= \lambda C(\epsilon, h)
		\label{eqn:specific_heat_homogeneous}
	\end{align}
	We have used $\epsilon = \frac{T}{T_c} -1$ and $d\epsilon = \frac{1}{T_c} dT$. Now setting $h=0$ we get from \ref{eqn:specific_heat_homogeneous}
	\begin{align}
		\lambda^{2 a_\epsilon} C(\lambda^{a_\epsilon} \epsilon) &= \lambda C(\epsilon) \nonumber \\
		C(1) &= \lambda^{1- 2 a_\epsilon} C(\epsilon) \nonumber \\
			 &= \epsilon^{-\frac{1-2 a_\epsilon}{a_\epsilon}} C(\epsilon) \nonumber \\
		C(\epsilon) &= \epsilon^{-\alpha} C(1) \label{eqn:specific_heat_and_alpha}
	\end{align}
	We have used the value of $\lambda$ when we set $\epsilon \lambda^{a_\epsilon}=1$ and the exponent
	\begin{equation}
		\alpha = \frac{2 a_\epsilon - 1}{a_\epsilon}
		\label{eqn:alpha}
	\end{equation}
	
	Again by differentiating \ref{eqn:gibbs_homogenious} twice with respect to $h$ we get the susceptibility. Then we set $h=0$ to get only $\epsilon$ dependency.
	\begin{align}
		\lambda \frac{\partial^2 G(\epsilon,h)}{\partial h^2} &= \lambda^{2 a_h} \frac{\partial^2 G(\lambda^{a_\epsilon} \epsilon, \lambda^{a_h} h)}{\partial h^2} \nonumber \\
		\lambda \chi(\epsilon, h) &= \lambda^{2 a_h} \chi(\lambda^{a_\epsilon}\epsilon, \lambda^{a_h} h) \nonumber \\
		\chi(\epsilon) &= \lambda^{2 a_h - 1} \chi(\lambda ^{a_\epsilon} \epsilon) \nonumber \\
		\chi(\epsilon) &= \epsilon^{-\gamma} \chi(1) 
		\label{eqn:susceptibility_homogeneous}
	\end{align}
	Similar to previous case, We have used the value of $\lambda$ when we set $\epsilon \lambda^{a_\epsilon}=1$ and the exponent is
	\begin{equation}
		\gamma = \frac{2 a_h -1}{a_\epsilon}
		\label{eqn:gamma}
	\end{equation}
	\subsection{List of Thermodynamic Quantities that Follows Power Law}
	\label{subsect:list-of-exponents}
	
		The exponent that scales Specific heat is called $\alpha$ 
		\begin{equation}
			C \sim \epsilon^{-\alpha}
		\end{equation}
		The exponent that scales order-parameter is called $\beta$ 
		\begin{equation}
			m \sim \epsilon^{\beta}
		\end{equation}
		Another exponent that scales order-parameter is called $\delta$, but it relates the order parameter with the magnetic field, $h$
		\begin{equation}
			m \sim h^{\delta}
		\end{equation}
		The exponent that scales susceptibility is called $\gamma$ 
		\begin{equation}
			\chi \sim \epsilon^{-\gamma}
		\end{equation}
		Note that these quantities only follows power law near $T_c$.
	\subsection{Rushbrooke Inequality}
		\begin{equation}
			\alpha + 2 \beta + \gamma \ge 2
		\end{equation}
		but the equality is often obtain theoretically which is shown below in the present case 
		\begin{align}
			\alpha + 2\beta + \gamma &= \frac{2 a_\epsilon -1}{a_\epsilon} 
					+ \frac{2 (1 - a_h)}{a_\epsilon}
					+ \frac{2 a_h - 1}{a_\epsilon} \nonumber \\
					&= \frac{2 a\epsilon}{a\epsilon} \nonumber \\
					&= 2 \nonumber
		\end{align}
	Thus the Rushbrooke inequality is satisfied.
	\subsection{Griffiths Inequality}
	\begin{equation}
		\alpha + \beta (1+\delta)  = 2
	\end{equation}
	Let's do a quick check to see if this is also satisfied.
	\begin{align}
		\alpha + \beta (1+\delta) &= \frac{2 a_\epsilon - 1}{a_\epsilon} 
									+ \frac{1 - a_h}{a_\epsilon} \left(1 + \frac{a_h}{1 - a_h}\right) \nonumber \\
								  &= \frac{2 a_\epsilon - 1}{a_\epsilon}  + \frac{1}{a_\epsilon} \nonumber \\
								  &= \frac{2 a_\epsilon}{a_\epsilon} \nonumber \\
								  &= 2 \nonumber \\
	\end{align}
	So the Griffiths Inequality is also satisfied. 
	
	
	
\section{Ising Model}
	For a very long time scientists have been trying to understand phase transition in different aspects. And for this purpose many different models have been provided for decades. The first model was provided by physicist Ernst Ising as a mathematical model of ferromagnetism in statistical mechanics. 
	The Ising model was invented by the physicist Wilhelm Lenz (1920), who gave it as a problem to his student Ernst Ising. The one-dimensional Ising model has no phase transition and was solved by Ising (1925) himself in his 1924 thesis \cite{Ising1925}.
	But he provided a model in 1 dimension and proved that in 1 dimension phase transition is not possible and concluded that it is not possible in higher dimension as well. But it was proved later that in dimension $d \geq 2$ phase transition is possible \cite{Yang1952, Montroll1963} which proved Ising wrong about his conclusion but he certainly gave a good insight for phase transition and hence the naming.
	
	Consider a set $\Lambda$ of lattice sites, each with a set of adjacent sites forming a d-dimensional lattice. For each lattice site $k\in\Lambda$ there is a discrete variable $\sigma_k$ such that $\sigma_k \in \{+1, -1\}$, representing the site's spin. A spin configuration $\sigma = \left(\sigma_k\right)_{k\in\Lambda}$ is an assignment of spin value to each lattice site.
	
	For any two adjacent sites $i,j \in \Lambda$ there is an interaction $J_{ij}$. Also a site $j\in \Lambda$ has an external magnetic field $h_j$ interacting with it. The energy of a configuration $\sigma$ is given by the Hamiltonian
	\begin{equation}
		H(\sigma) = - \sum_{<i j>} J_{ij} \sigma_i \sigma_j - \mu \sum_{j} h_j \sigma_j
	\end{equation}
	Where the first sum is over pairs of adjacent spins counting every pair only once. And we use $<ij>$ to indicate that $i$ and $j$ are nearest neighbors. $\mu$ indicates the magnetic moment. The negative sign on the second term is there by convention \cite{Baierlein1999}. The configuration probability is given by the Boltzmann distribution with inverse temperature $\beta \geq 0$
	\begin{equation}
		P_\beta (\sigma) = \frac{e^{- \beta H(\sigma)}}{Z_\beta}
	\end{equation}
	where $\beta = \left(k_B T\right)^{-1}$ and the normalization constant
	\begin{equation}
		Z_\beta = \sum_\sigma e^{-\beta H(\sigma)}
	\end{equation}
	is the partition function. For a function $f$ of the spins, which is the observable here, we denote
	\begin{equation}
		<f>_{\beta} = \sum_{\sigma} f(\sigma) P_\beta(\sigma)
	\end{equation}
	the expectation value of $f$.
	The configuration probabilities $P_\beta(\sigma)$ represent the probability that the system in in a state with configuration $\sigma$ in equilibrium.
	
	
	The minus sign on each term of the Hamiltonian function $H(\sigma)$ is conventional. Using this sign convention, the Ising models can be classified according to the sign of the interaction: if, for all pairs $i,j$
	\begin{equation}
	J_{ij}
	\begin{cases}
		 > 0 \text{, ferromagnetic interaction }\\
		 < 0 \text{, anti-ferromagnetic interaction}\\
		 = 0 \text{, non-interacting spins}
	\end{cases}
	\end{equation}
	
	otherwise the system is called non-ferromagnetic.
	
	In a ferromagnetic Ising model, spins tend to be aligned, meaning, the configurations in which adjacent spins are of the same sign have higher probability. In an anti-ferromagnetic model, adjacent spins tend to have opposite signs.
	
	The sign convention of $H(\sigma)$ also explains how a spin site $j$ interacts with the external field. Namely, the spin site wants to line up with the external field.
	
	\begin{equation}
	h_j
	\begin{cases}
		 > 0 \text{,  the spin site $j$ desires to line up in the positive direction} \\
		 > 0 \text{,  the spin site $j$ desires to line up in the negative direction} \\
		 = 0 \text{,  no external influence on the spin site}
	\end{cases}
	\end{equation}



	Ising models are often examined without an external field interacting with the lattice, that is, $h = 0$ for all $j$ in the lattice $\Lambda$. Using this simplification, our Hamiltonian becomes:
	\begin{equation}
		H(\sigma) = - \sum_{<i j>} J_{i j} \sigma_i \sigma_j
	\end{equation}
	When the external field is everywhere zero, $h = 0$, the Ising model is symmetric under switching the value of the spin in all the lattice sites; a non zero field breaks this symmetry.
	
	Another common simplification is to assume that all of the nearest neighbors $<ij>$ have the same interaction strength. Then we can set $J_{ij} = J$ for all pairs $i,j$. In this case our Hamiltonian is further simplified to:
	
	\begin{equation}
		H(\sigma) = - J \sum_{<i j>} \sigma_i \sigma_j
	\end{equation}
	
	Ising himself had provided the theory of his model for $1$ dimensional case. He did not find any phase transition and concluded that phase transition is not possible in any dimension. 
	
	Consider an one dimensional lattice where all sites are in spin up state. Then according to Ising model their total energy $E$ and entropy $S$ is as follows
	\begin{align}
		E &= -N J \\
		S &= 0
	\end{align}
	Where $N$ is the number of sites on the lattice. Now if one of the sites is fliped then
	\begin{align}
		E &= - (N-2)J \\
		S &= \log N
	\end{align}
	Therefore the cost is $\Delta E = 2J$ and change in entropy is $\Delta S = \log N$ for flipping spin of one site. The figure \ref{fig:1d-ising-model} shows this.
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[height=0.6cm]{{{ising-1d-0}}}
			\caption{all are in spin up state}
		\end{subfigure}
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[height=0.6cm]{{{ising-1d-1}}}
			\caption{only one site is in spin down state}
		\end{subfigure}
		\caption{1 dimensional Ising model}
		\label{fig:1d-ising-model}
	\end{figure}
	
	Therefore the change in free energy due to flipping one spin is,
	\begin{align}
		\Delta F &= \Delta E - T \Delta S \\
				 &= 2J - T \log N
	\end{align}
	In the thermodynamic limit $N \rightarrow \infty$ the second term always dominates except for $T=0$. Thus in one dimensional lattice there is phase transition at non zero temperature sice we cannot have $\Delta F=0$ (the transition point).
	
	
	Now interesting this happens in case of two dimensional lattice. Say we have a square lattice of size $N$ where there is an island of $L=3$ sites of different spins \ref{fig:2d-ising-model}.
	\begin{figure}
		\centering
		\includegraphics[width=5cm]{{{ising-model-2d}}}
		\caption{2D Ising model with an island of 3 site with opposite spin state}
		\label{fig:2d-ising-model}
	\end{figure}
	\begin{align}
		\Delta E &= 2 L J \\
		\Omega &= 3^L N \\
		S &= K \log \Omega \\
		\Delta S &= S - 0 = S
	\end{align}
	Therefore the free energy is 
	\begin{align}
		\Delta F 
		&= \Delta E - T\Delta S \\
		&= 2JL - KT\log (3^L N) \\
		&= 2 J L - T L \log 3 - K T \log N
	\end{align}
	since at transition point $\Delta F = 0$ we get
	\begin{align}
		0 &= L(2J - T_c \log 3) \\
		T_c &= \frac{2J}{\log 3}
	\end{align}
	clearly $J\neq 0$ then $T_c \neq 0$.
	The exact solution is 
	\begin{equation}
		T_c = \frac{2J}{\log(1 + \sqrt{2})}
	\end{equation}
	Thus we can see that spontaneous magnetizaion (phase transition) is possible in 2D for a non zero $T$. And any higher dimensional lattice undergoes a transition for a non zero temperature.
	
	
